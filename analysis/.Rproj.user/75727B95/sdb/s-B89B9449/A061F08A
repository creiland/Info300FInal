{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Using Machine Learning to Predict House Prices in King County (statistical analysis)\"\nauthor: \"Conor Reiland, Yusha Wang, Yu-Ting Chen\"\ndate: \"November 15, 2018\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n##**Goal:**\n*Our main goal is to make a predicting model for housing price in King County.* \\n        \n\nHouse pricing is an important economic metric, as well as an omnipresent worry for much of the population. So we want to predict housing price so that one person can know whether he/she can afford the price and whether should start saving money or buying a loan in order to purchase the house. Some of the similar questions have been asked before, but we will combine datasets and add several new columns, leading us to believe the results may highlight new patterns in the data. Our goal is to look at the data from a new perspective and try to solve problems in a new way. The final stage is unclear for now, but we can potentially study the factors and try to build a simplified model to predict future house prices in King County.\\n\n\n##**Read in Dataset:**\n```{r Loading data}\nlibrary(corrplot)\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(MASS)\nrequire(caret)\nlibrary(dplyr)\ndata = read.csv(\"clean_output.csv\")\n```\n##**Descriptive Statistics:**\n\n```{r summary statistics}\ndrop = c('id','date','long','lat','zipcode','neighborhood', 'X')\nneighborhood.f = as.factor(data$neighborhood)\ndata = data[,!(names(data) %in% drop)]\nsummary_data = do.call(cbind, lapply(data, summary))\nround(summary_data,2)\n```\n\\n\n\nTo begin with, some unrelated data columns are removed from the dataframe, such as the id numbers, the data that the house was regestered, the longtitude, the latitude and the zipcode,etc. The data frame above shows the descriptive statistic about the selling house and some characteristics of the buyer. For example, the price has an extensive range in King County from \\$75000 to \\$7700000 from the year 2014-2015 with an average price for $540088. The median income for people living in King County is \\$81694 during the same period. By comparing the median of the median income level and the housing price, results has shown that the median house price has outsripped the median of the median income level, which indicats a sersies problem that the housing price has become a heavy burden to the people who live in King County, and increase the probability that people choose to rent a house instead of purchasing one. Otherwise, there is a great chance that people will need to borrow money from the bank in order to own a house.\nOn the other hand, there are categorical features such as view, condition, and grade. The value of the grade is decided based on the King County's grading system, which is in the scale of 1 to 13. The median value for the houses' grading is seven, which means the quality of the house in King County are averagely higher than expected, since the median of the entire grading system is 6.5. Same situation happens in another variable, condition. For the variable \"view\" in the dataset, results has shown that the median value is 0 and the max value is 4, which means that people seldom view the house in person and they may choose to view the conditions of the houses online or buy the houses only based on the recommendations of other people.   \\n\n\n\n```{r multiplot, echo = FALSE}\npar(mfrow=c(3,3))\nplot(data$bedrooms, data$price, main = \"Price and Bedrooms\", xlab = \"Bedrooms\", ylab =\"Price\")\nabline(lm(data$price ~ data$bedrooms),col = 2)\nplot(data$bathrooms, data$price,  main = \"Price and Bathrooms\", xlab = \"Bathrooms\", ylab =\"Price\")\nabline(lm(data$price ~ data$bathrooms),col = 2)\nplot(data$sqft_living, data$price,  main = \"Price and sqft of Living room\", xlab = \"sqft of Living room\", ylab =\"Price\")\nabline(lm(data$price ~ data$sqft_living),col = 2)\nplot(data$sqft_lot, data$price, main = \"Price and sqft of Lot\", xlab = \"sqft of Lot\", ylab =\"Price\")\nabline(lm(data$price ~ data$sqft_lot),col = 2)\nplot(data$floors, data$price, main = \"Price and Floors\", xlab = \"Floors\", ylab =\"Price\" )\nabline(lm(data$price ~ data$floors),col = 2)\nplot(data$condition, data$price, main = \"Price and Condition\", xlab = \"condition\", ylab =\"Price\")\nabline(lm(data$price ~ data$condition),col = 2)\nplot(data$waterfront, data$price, main = \"Price and Waterfront\", xlab = \"Waterfront\", ylab =\"Price\")\nabline(lm(data$price ~ data$waterfront),col = 2)\nplot(data$view, data$price, main = \"Price and View\", xlab = \"View\", ylab =\"Price\")\nabline(lm(data$price ~ data$view),col = 2)\nplot(data$grade, data$price, main = \"Price and Grade\", xlab = \"Grade\", ylab =\"Price\")\nabline(lm(data$price ~ data$grade),col = 2)\nplot(data$sqft_above, data$price, main = \"Price and sqft Above\", xlab = \"sqft Above\", ylab =\"Price\")\nabline(lm(data$price ~ data$sqft_above),col = 2)\nplot(data$sqft_basement, data$price, main = \"Price and sqft Basement\", xlab = \"sqft Basement\", ylab =\"Price\")\nabline(lm(data$price ~ data$sqft_basement),col = 2)\nplot(data$yr_built, data$price, main = \"Price and Year Built\", xlab = \"Year Built\", ylab =\"Price\")\nabline(lm(data$price ~ data$yr_built),col = 2)\nplot(data$yr_renovated, data$price, main = \"Price and Year Renovated\", xlab = \"Year Renovated\", ylab =\"Price\")\nabline(lm(data$price ~ data$yr_renovated),col = 2)\nplot(data$sqft_living15, data$price, main = \"Price and Living room area in 2015\", xlab = \"Living room area in 2015\", ylab =\"Price\")\nabline(lm(data$price ~ data$sqft_living15),col = 2)\nplot(data$sqft_lot15, data$price,  main = \"Price and Lot size area in 2015\", xlab = \"Lot size area in 2015\", ylab =\"Price\")\nabline(lm(data$price ~ data$sqft_lot15),col = 2)\nplot(data$median_income_by_zip, data$price, main = \"Price and Median Income\", xlab = \"Median Income\", ylab =\"Price\")\nabline(lm(data$price ~ data$median_income_by_zip),col = 2)\nplot(data$mean_income_by_zip, data$price, main = \"Price and Mean Income\", xlab = \"Mean Income\", ylab =\"Price\")\nabline(lm(data$price ~ data$mean_income_by_zip),col = 2)\nplot(data$population_by_zip, data$price, main = \"Price and Population\", xlab = \"Population\", ylab =\"Price\")\nabline(lm(data$price ~ data$population_by_zip),col = 2)\nplot(data$people_employed_by_zip, data$price,  main = \"Price and Employed People\", xlab = \"Employed People\", ylab =\"Price\")\nabline(lm(data$price ~ data$people_employed_by_zip),col = 2)\nplot(data$people_educated_by_zip, data$price, main = \"Price and Educated People\", xlab = \"Educated People\", ylab =\"Price\")\nabline(lm(data$price ~ data$people_educated_by_zip),col = 2)\nplot(data$bachelor_degrees_by_zip, data$price, main = \"Price and Number of Bachelor Degrees\", xlab = \"Bachelor Degrees\", ylab =\"Price\")\nabline(lm(data$price ~ data$bachelor_degrees_by_zip),col = 2)\nplot(data$grad_degrees_by_zip, data$price, main = \"Price and Number of Graduate Degrees\", xlab = \"Graduate Degrees\", ylab =\"Price\")\nabline(lm(data$price ~ data$grad_degrees_by_zip),col = 2)\n```\n\nIn the scatter plot of price VS sqrt_living, almost all the dots are a cluster in the left bottom corner, but they seem to have a positive correlation. This is reasonable since if the living space has increased, the price of the house should increase as well. There is some extreme dataset show in the plot, which is far from the best fit line generated by the code. This indicates that the relationships between the house price and the space for a living are not absolute, and other factors may still influence the house price more than the space for living under some conditions. For example, when comparing a house with larger spaces in the rural area or a house with a small area in the center of the city, house buyers may prefer the house in the city center more rather than the house in the rural area. As a result, the house seller may sell the house at the city center with a higher price.\\n\n\nSome variables's data are collected based on the zipcode, so they may share some similar features on their scatter plots. For instance, in the scatter plot of price v.s. mean_income , the mean income is separated by zip code, which means that many data should share the same mean income since they have the same zip code, and this will make the data that distributed on the scatter plots seems like numerous vertical lines. This also shows another problem, even under the average income levels, house price still varies a lot, which makes it hard to use the mean income as an indicator of the house price even though the correlation is around 0.5. In consequence, for other variables that have been collected based on the zipcode, it is normal to find out if such variables doesn't show a high correlation with the housing price..\\n\n\n\n```{r correlation matrix}\nmatrix = cor(data, use = \"pairwise.complete.obs\")\nround(matrix,2)\n```\nThis matrix above shows the correlation among all the explanatory. At first glance, most of the data do not correlate. So, we will use this matrix to generate a correlation plot which can demonstrate the strong and weak relationships. \\n\n```{r Correlation plot}\ncorrplot(matrix, type = \"upper\", order = \"hclust\", \n         tl.col = \"black\", tl.srt = 60)\n\n```\n\\n \n\nIn the correlation plot, some variables share a strong correlation, such as the median income and the mean income; the number of people who earned the graduate degree and the number of people who earned a bachelor degree; number of people who are employed and number of people who attend school in 2011 - 2015, etc. This dataset has run into a multicollinearity problem, which may have the following drawbacks:\\n \n\n**(a)** Parts of the coefficients of the regression may not be precisely estimated due to multicollinearity.\\n\n\n**(b)** The standard errors are likely to be high.\\n\n\n**(c)** In different samples, multicollinearity may change the sign and the magnitude of each coefficient.\\n\n\n**(d)** It is meaningless to assess the relative importance of the independent variables in explaining the variation caused by the dependent variable.\n\\n\n\nAlthough there are no any two variables result in perfect collinearity, there are still some explanatory variables share the correlation around 0.9. In order to lower the chance of generating problems because of collinearity, some of the variables will be taken off from the analysis.\\n  \n\n```{r Lower collinearity}\nmatrix[upper.tri(matrix)] <- 0\ndiag(matrix) <- 0\ndata.new <- data[,!apply(matrix,2,function(x) any(x > 0.9))]\n#head(data)\n#ncol(data)\n```\n\\n\n\nAfter removing the variables that share correlations above 0.9, only 19 explanatory variables are left, and the correlations of these variables must be rechecked:\\n\n```{r Check matrix again}\nmatrix2 = cor(data, use = \"pairwise.complete.obs\")\nround(matrix2,2)\n```\n```{r correplot2}\ncorrplot(matrix2, type = \"upper\", order = \"hclust\", \n         tl.col = \"black\", tl.srt = 60)\n```\n\\n \n\nFrom the second correlation plot, some changees can be detected. After taking out some of the explanatory variables that share high correlations, the multicollinearity problem is improved , which allow us to increase the precision of machine learning models to some degree.  \n \\n\n\n##**AIC model Forward Selection:**\n```{r AIC model selection (forward)}\n#convert categorical variables to factors\ndata.new$waterfront = as.factor(data.new$waterfront)\ndata.new$yr_built = as.factor(data.new$yr_built)\ndata.new$view = as.factor(data.new$view)\ndata.new$grade = as.factor(data.new$grade)\ndata.new$condition = as.factor(data.new$condition)\n\n#fit AIC forward selection\nfit_full_rw = lm(log_p~., data=data.new)\nfit_null_rw = lm(log_p~1, data=data.new)\nF_fit_rw = step(fit_null_rw, scope=list(lower=fit_null_rw,upper=fit_full_rw),\n                direction=\"forward\")\n\nsummary(F_fit_rw)\n\n#add columnds for predicted log_p and standard residuals\ndata.new <- na.omit(data.new) %>% mutate(log_p_predicted = predict(F_fit_rw)) %>% mutate(stdresid = rstandard(F_fit_rw))\n\n#plot predicted v actual\nexpected_actual <- ggplot(data.new, aes(x = log_p_predicted, y = log_p)) + geom_point() +\n                    labs(title = \"Expected vs Actual for Forward Selection\", x=\"Predicted log_p\", y='Actual log_p')\n\n#plot standard residuals against predicted values\nstdresid <- ggplot(data.new, aes(x = log_p_predicted, y = stdresid)) + geom_point() +\n                    labs(title = \"Standardized Residuals for Forward Selection\", x=\"Predicted log_p\", y='Standardized Residual')\n\n#qqpplot for standard residuals\nqq <- qqnorm(data.new$stdresid, main=\"Normal QQ Plot for log_p Standard Residuals\", xlab=\"Theoretical\", ylab=\"Standard Residual\")\nplot(qq)\nabline(mean(data.new$stdresid), sd(data.new$stdresid))\n```\n\\n\n\nIn this section, the AIC model selection method is applied instead of the R square. The drawback of the R square is that as the number of variables increase, the R square will increase. Moreover, R square is a measure of explained variance in the response variable by the predictors, and it does not show the sign of a better model. On the other hand, the AIC model uses the negative log likelihood in order to penalize for many parameters, which means that if a model fits the data better, it will have a lower value. By setting the upper limit of the AIC value when all of the variables are included, and the lower limit by none of the explanatory variable is included, results have shown the model that best fits the data and this models generated the smallest AIC model: $$price = \\beta_0 +\\beta_1\\times \\textrm{sqrt_living} +\\beta_2 \\times \\textrm{graduate degrees} + \\beta_3 \\times \\textrm{people_educated} + \\beta_4 \\times{view} $$ \\n\n$$\n+ \\beta_5 \\times \\textrm{waterfront} +\\beta_6 \\times \\textrm{grade} + \\beta_7 \\times \\textrm{year built} +\\beta_8 \\times \\textrm{mean income} + \\beta_9 \\times \\textrm{bedrooms}$$\\n\n$$\n+\\beta_{10}\\times \\textrm{bathrooms} + \\beta_{11} \\times \\textrm{sqft_lot15} + \\beta_{12} \\times \\textrm{condition} + \\beta_{13} \\times \\textrm{sqft_above}$$\\n\n$$ + \\beta_{14} \\times \\textrm{ yr_renovated} + \\beta_{15} \\times \\textrm{sqft_living15} + \\beta_{16} \\times \\textrm{floors} + \\beta_{17} \\times \\textrm{sqft_lot} $$ \\n\n \nThere are 16 explanatory in our multi-linear model and 17 beta coefficients. When we set all explanatory to zero, we will get an interception at $beta_0$, the value of price. When we increase the $X_i$ explanatory such as sqrt_living by one unit and keep other explanatory remain the same, our price will increases by a $beta_i$ unit on average such as $beta_1$.\\n\n\n##**Normalize Price:**\nLet's check out the most interesting feature in this study: Sale Price. \\n\n```{r Graph}\nhist(data$price, main = \"Histogram of Price\", xlab = \"Price\", col = \"orange\")\n```\n\n```{r Graph2}\nqqnorm(data$price)\nabline(mean(data$price), sd(data$price))\n```\n\\n\n\nIn this part, the normality of the price data has been estimated. From the histogram, the price data has been shown to have a right tail and skewed to the right. Using the qq plot to estimate the normailty, it is observed that a large portion of data on the right tail deviate from the line, which confirm the data don't have a normal distribution.\\n\n\n```{r Graph3}\nh = hist(log(data$price), main = \"Histogram of Price\", xlab = \"Price\",col = \"orange\", ylim = c(0,8000))\nx = seq(11, 16, by = 0.05)\nxfit <- seq(min(log(data$price)), max(log(data$price)), length = 40) \nyfit <- dnorm(xfit, mean = mean(log(data$price)), sd = sd(log(data$price))) \nyfit <- yfit * diff(h$mids[1:2]) * length(log(data$price)) \n\nlines(xfit, yfit, col = \"black\", lwd = 2)\n```\n\nSince the resoponse value(Price) is skewed, the log transfomation is applied in order to ajust the data to be close to noremal distribution. After log transformation, our price feature looks like a normal distribution. The skewed data is being adjusted to be more normal so that the models will be more accurate when making predictions.\\n\n\n```{r Regression}\nregression = lm(log(price) ~ sqft_living + grad_degrees_by_zip + people_educated_by_zip + \n    view + waterfront + grade + yr_built + mean_income_by_zip + \n    bedrooms + bathrooms + sqft_lot15 + condition + sqft_above + \n    yr_renovated + sqft_living15 + floors + sqft_lot, data = data)\nsummary(regression)\n```\n\\n\n\nIn the regression model, coefficients of sqft_living, grad_degrees_by_zip, view, waterfront, grade, mean_income_by_zip, bathrooms, condition, yr_renovated, sqft_living15, floors, and sqft_lot are positive. This indicates a unit increases in each of the above explanatory variables will lead to an increase in price on a log scale by the amount of their coefficient on average, keeping other unchanged. Coefficients of people_educated_by_zip, yr_built, bedrooms, sqft_lot15, and sqft_above are negative. This indicates a unit increases in each of the above explanatory variables will lead to a decrease in price on a log scale by the amount of their coefficient on average, keeping other unchanged. In other word, if we change one explanatory by one unit, then our predictor price will change by 100*$beta_i$ percent. When all the explanatory take zeros, the house price will be 16.15.\\n\n\nBy comparing at the P-value and significant level, except bedrooms and sqdt_above, all the explanatory are statistcally significant at the significance level of 0.05, since their P-value is less than 0.05. The same conclusion can be reached when showing the overall P value. In another word, the null hypothesis that these explanatory variables have no effect on the price is rejected in favor of the alternative hypothesis, and some of the variables do has significant influces on the price level.\\n\n",
    "created" : 1544581101441.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1765224943",
    "id" : "A061F08A",
    "lastKnownWriteTime" : 1543891337,
    "last_content_update" : 1543891337,
    "path" : "C:/Users/creil/Desktop/Info370/Final/Statistical Analysis/statistic_analysis_v3.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}